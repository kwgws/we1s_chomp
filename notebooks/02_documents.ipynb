{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # STEP 2: SCRAPE DOCUMENTS\n",
    "\n",
    " <h4>This notebook will collect document metadata from scraped responses.</h4>\n",
    "\n",
    "It supports the Wordpress API (reference here: https://developer.wordpress.org/rest-api/) and the Google Custom Search Engine API (reference here: https://developers.google.com/custom-search/v1/) out of the box, though other APIs can be added. For more information and further instructions, consult the Chomp documentation at https://github.com/seangilleran/we1s_chomp."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## INFO\n",
    " \n",
    "__authors__    = 'Sean Gilleran'  \n",
    "__copyright__  = 'copyright 2019, The WE1S Project'  \n",
    "__license__    = 'MIT'  \n",
    "__version__    = '0.1.0'  \n",
    "__email__      = 'sgilleran@ucsb.edu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## SETTINGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import getenv\n",
    "from pathlib import Path\n",
    "\n",
    "from we1s_chomp import google, wordpress\n",
    "from we1s_chomp.model import Document\n",
    "from we1s_chomp.web import Browser\n",
    "\n",
    "project_dir = Path.home() / \"write\" / \"dev\" / \"we1s_chomp\"\n",
    "url_stopwords_file = project_dir / \"notebooks\" / \"url_stopwords.txt\"\n",
    "\n",
    "grid_url = getenv(\"CHOMP_SELENIUM_GRID_URL\")\n",
    "\n",
    "url_stops = set()\n",
    "\n",
    "# Get stopwords.\n",
    "url_stopwords = set()\n",
    "with open(url_stopwords_file, encoding=\"utf-8\") as txtfile:\n",
    "    for line in txtfile.readlines():\n",
    "        stopword = line.strip()\n",
    "        if stopword != \"\":\n",
    "            url_stopwords.add(stopword)\n",
    "            print(f'Added URL stopword: \"{stopword}\".')\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## DATA DIRECTORIES\n",
    "\n",
    " Chomp will import and export JSON files to these directories, using them not\n",
    " only to set up and store the results of a collection run and but also to keep\n",
    " track of metadata, duplicate URLs, page numbers, etc. as it goes.\n",
    "\n",
    " <p style=\"color:red;\">Because collection is a time- and resource-intensive\n",
    " task, it is preferable to keep these metadata files in a single location and\n",
    " allow Chomp to manage them internally rather than modifying or deleting them\n",
    " between each job.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_dir = project_dir / \"data\" / \"json\" / \"queries\"\n",
    "source_dir = project_dir / \"data\" / \"json\" / \"sources\"\n",
    "response_dir = project_dir / \"data\" / \"json\" / \"responses\"\n",
    "document_dir = project_dir / \"data\" / \"json\" / \"documents\"\n",
    "\n",
    "# Make document directory if it does not already exist.\n",
    "if not document_dir.exists():\n",
    "    document_dir.mkdir(parents=True)\n",
    "\n",
    "print(f\"Loading sources from {source_dir}.\")\n",
    "print(f\"Loading queries from {query_dir}.\")\n",
    "print(f\"Loading responses from {response_dir}.\")\n",
    "print(f\"Saving documents to {document_dir}.\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## BROWSE: Search responses for keywords\n",
    "\n",
    "Choose `search_text` to filter available response files. If you are searching for a specific word or phrase, enter it WITHIN the single quotes below. Note that you will be searching the filenames of the JSON files stored on in the `response_dir` path (usually `data/json/responses`). If you want to simply list all of the available responses, change the value of the `search_text` variable below to `None` WITHOUT single quotes (so the line should read `search_text = None`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_text = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell and review the results. The default is to search through the `data/json/responses` directory. If your data is in a different location on harbor, change the `response_dir` variable above to the directory you want to search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "print(\"response_list = [\")\n",
    "for filename in response_dir.glob(\"*.json\"):\n",
    "    with open(filename, encoding=\"utf-8\") as jsonfile:\n",
    "        name = json.load(jsonfile).get(\"name\", \"\")\n",
    "    if not name or \"chomp-response\" not in name:\n",
    "        continue\n",
    "    print('    \"' + name + '\",')\n",
    "print(\"]\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LIST: Define which queries will be scraped\n",
    "\n",
    "Copy the entire cell output above and replace the `response_list` array in the following cell. Each response name should be surrounded by quotes, and after each name there should be a comma (for the last filename in the list it doesn't matter if you include the comma or not).\n",
    "\n",
    "Don't forget to run the cell!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_list = [\n",
    "    \"chomp-response_we1s_humanities_2014-01-01_2019-12-31_0\",\n",
    "    \"chomp-response_we1s_humanities_2014-01-01_2019-12-31_1\",\n",
    "    \"chomp-response_we1s_humanities_2014-01-01_2019-12-31_2\",\n",
    "    \"chomp-response_we1s_humanities_2014-01-01_2019-12-31_3\",\n",
    "    \"chomp-response_we1s_humanities_2014-01-01_2019-12-31_4\",\n",
    "    \"chomp-response_we1s_humanities_2014-01-01_2019-12-31_5\",\n",
    "    \"chomp-response_we1s_humanities_2014-01-01_2019-12-31_6\",\n",
    "    \"chomp-response_we1s_humanities_2014-01-01_2019-12-31_7\",\n",
    "    \"chomp-response_libcom-org_humanities_2000-01-01_2019-12-31_0\",\n",
    "    \"chomp-response_libcom-org_humanities_2000-01-01_2019-12-31_1\",\n",
    "    \"chomp-response_libcom-org_humanities_2000-01-01_2019-12-31_2\",\n",
    "    \"chomp-response_libcom-org_humanities_2000-01-01_2019-12-31_3\",\n",
    "    \"chomp-response_libcom-org_humanities_2000-01-01_2019-12-31_4\",\n",
    "    \"chomp-response_libcom-org_humanities_2000-01-01_2019-12-31_5\",\n",
    "    \"chomp-response_libcom-org_humanities_2000-01-01_2019-12-31_6\",\n",
    "    \"chomp-response_libcom-org_humanities_2000-01-01_2019-12-31_7\",\n",
    "    \"chomp-response_libcom-org_humanities_2000-01-01_2019-12-31_8\",\n",
    "    \"chomp-response_libcom-org_humanities_2000-01-01_2019-12-31_9\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## IMPORT & TEST RESPONSES\n",
    "\n",
    " Let's take a second here to make sure our responses are all in good shape--\n",
    " that the dates are all correct and that they all connect to a proper source.\n",
    " That way there won't be any surprises later.\n",
    "\n",
    " Run this cell and check for any errors in the output. Make sure that the\n",
    " number of responses imported is equal to the number of responses you intended\n",
    " to import."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from we1s_chomp import clean, db\n",
    "\n",
    "responses = []\n",
    "for response_name in response_list:\n",
    "    response = db.load_response(response_name, response_dir)\n",
    "    if not db.load_source(response.source, source_dir):\n",
    "        print(f'WRN: \"{response.source}\" not found, skipping \"{response.name}\".')\n",
    "        continue\n",
    "    if not db.load_query(response.query, query_dir):\n",
    "        print(f'WRN: \"{response.query}\" not found, skipping \"{response.name}\".')\n",
    "        continue\n",
    "    responses.append(response)\n",
    "    print(f'Imported \"{response.name}\".')\n",
    "print(f\"{len(responses)} responses imported out of {len(response_list)} total.\")\n",
    "if len(responses) == len(response_list):\n",
    "    print(\"Everything looks good so far!\\n\\n\")\n",
    "else:\n",
    "    print(\"Hmm, does that seem right to you? Double-check!\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## LOAD URL STOPS (Optional)\n",
    "\n",
    " Load previously collected URLs. Skipping this step will force all responses to be re-collected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for response in responses:\n",
    "    for document_name in response.documents:\n",
    "        document = db.load_document(document_name, document_dir)\n",
    "        url_stops.add(document.url)\n",
    "print(f\"Added {len(url_stops)} URLs to URL stop list.\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # GET DOCUMENTS\n",
    "\n",
    " Use the queries to start scraping responses.\n",
    " \n",
    " <h3 style=\"color:red;font-weight:bold\">Pay close attention to errors here--many things can go wrong when dealing with the web!</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture output\n",
    "%%time\n",
    "total = 0\n",
    "\n",
    "# Start browser connection. We need this for Google documents.\n",
    "browser = Browser(grid_url)\n",
    "\n",
    "# We need to restart document count whenever we get a new query; that means\n",
    "# keeping track of the old query.\n",
    "last_query = db.load_query(responses[0].query, query_dir)\n",
    "count = no_exact_match_count = 0\n",
    "\n",
    "for response in responses:\n",
    "\n",
    "    # Load associated source & query.\n",
    "    source = db.load_source(response.source, source_dir)\n",
    "    query = db.load_query(response.query, query_dir)\n",
    "    \n",
    "    # Reset count on new query.\n",
    "    if last_query.name != query.name:\n",
    "        count = no_exact_match_count = 0\n",
    "\n",
    "    # Select collection API.\n",
    "    api = response.chompApi\n",
    "\n",
    "    # Wordpress API ##########################################################\n",
    "    if api == \"wordpress\":\n",
    "        print(f'\\nCollecting \"{response.name}\" via Wordpress API...')\n",
    "        documents_raw = wordpress.get_metadata(\n",
    "            response=response.content,\n",
    "            query_str=query.query_str,\n",
    "            start_date=query.start_date,\n",
    "            end_date=query.end_date,\n",
    "            url_stops=url_stops,\n",
    "            url_stopwords=url_stopwords\n",
    "        )\n",
    "\n",
    "    # Google API #############################################################\n",
    "    else:\n",
    "        print(f'\\nCollecting \"{response.name}\" via Google API...')\n",
    "        documents_raw = google.get_metadata(\n",
    "            response=response.content,\n",
    "            query_str=query.query_str,\n",
    "            start_date=query.start_date,\n",
    "            end_date=query.end_date,\n",
    "            url_stops=url_stops,\n",
    "            url_stopwords=url_stopwords,\n",
    "            browser=browser,\n",
    "        )\n",
    "\n",
    "    if not documents_raw:\n",
    "        print(\"ERR: No results or connection error!\")\n",
    "        continue\n",
    "\n",
    "    # Loop over each document we get back and save it.\n",
    "    for doc in documents_raw:\n",
    "        if not doc:\n",
    "            print(\"WRN: No document found, skipping.\")\n",
    "            continue\n",
    "\n",
    "        # Parse result.\n",
    "        name = \"_\".join(\n",
    "            [\n",
    "                \"chomp\",\n",
    "                query.name,\n",
    "                str(count if not doc[\"no_exact_match\"] else no_exact_match_count)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # Documents that do not explicitly contain the search term may require\n",
    "        # special handling--set them aside.\n",
    "        if doc[\"no_exact_match\"]:\n",
    "            name += \"(no-exact-match)\"\n",
    "\n",
    "        document = Document(\n",
    "            name=name,\n",
    "            title=doc[\"title\"],\n",
    "            shortTitle=name,\n",
    "            chompApi=api,\n",
    "            url=doc[\"url\"],\n",
    "            pub_date=doc[\"pub_date\"],\n",
    "            content_html=doc[\"content_html\"],\n",
    "            content=doc[\"content\"],\n",
    "            pub=source.title,\n",
    "            copyright=source.copyright,\n",
    "            source=source.name,\n",
    "            query=query.name,\n",
    "            response=response.name,\n",
    "        )\n",
    "\n",
    "        if not doc[\"no_exact_match\"]:\n",
    "            count += 1\n",
    "        else:\n",
    "            no_exact_match_count += 1\n",
    "        total += 1\n",
    "        db.save_document(document, document_dir)\n",
    "\n",
    "        # Update response.\n",
    "        response.documents.add(document.name)\n",
    "        db.save_response(response, response_dir)\n",
    "\n",
    "        # Update query.\n",
    "        query.documents.add(document.name)\n",
    "        db.save_query(query, query_dir)\n",
    "\n",
    "        # Update source.\n",
    "        source.documents.add(document.name)\n",
    "        db.save_source(source, source_dir)\n",
    "        \n",
    "        # Save previous query.\n",
    "        last_query = query\n",
    "\n",
    "        print(f\"- {document.url}\")\n",
    "    print(f\"Done! Got {count + no_exact_match_count} documents from this response.\\n\\n\")\n",
    "print(f\"\\nAll responses complete! Got a total of {total} documents.\\n\\n\")\n",
    "print(\"\\n\\n----------Time----------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## NEXT NOTEBOOK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: next notebook code\n",
    "# Go to 03_export.ipynb"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
