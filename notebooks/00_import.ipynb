{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # IMPORT CSV CONFIGURATION\n",
    "\n",
    " This notebook will source and query metadata from CSV files, which was\n",
    " standard in older iterations of the Chomp software, and translate them into\n",
    " the JSON format used internally by more recent versions.\n",
    "\n",
    " For more information and further documentation and instructions, consult the\n",
    " Chomp documentation at https://github.com/kwgws/we1s_chomp."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## INFO\n",
    "  \n",
    "__authors__    = 'Catherine Gilleran'  \n",
    "__copyright__  = 'copyright 2019, The WE1S Project'  \n",
    "__license__    = 'MIT'  \n",
    "__version__    = '0.1.0'  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## SETTINGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "project_dir = Path.home() / \"write\" / \"dev\" / \"we1s_chomp\"\n",
    "source_dir = project_dir / \"data\" / \"json\" / \"sources\"\n",
    "query_dir = project_dir / \"data\" / \"json\" / \"queries\"\n",
    "\n",
    "if not source_dir.exists():\n",
    "    source_dir.mkdir(parents=True)\n",
    "if not query_dir.exists():\n",
    "    query_dir.mkdir(parents=True)\n",
    "    \n",
    "print(f\"Loading sources from {source_dir}.\")\n",
    "print(f\"Loading queries from {query_dir}.\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## CSV FILES\n",
    "\n",
    " Select files to import. Default import location is the `import` directory.\n",
    "  Set `None` to skip if you only one of the two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sources_csv_files = [\n",
    "    project_dir / \"data\" / \"import\" / \"sources.csv\",\n",
    "]\n",
    "\n",
    "queries_csv_files = [\n",
    "    project_dir / \"data\" / \"import\" / \"queries.csv\",\n",
    "]\n",
    "\n",
    "# Check files\n",
    "all_ok = True\n",
    "if sources_csv_files is not None:\n",
    "    for source_file in sources_csv_files:\n",
    "        if not source_file.exists():\n",
    "            print(f\"ERR: {source_file} does not exist.\")\n",
    "            all_ok = False\n",
    "if queries_csv_files is not None:\n",
    "    for query_file in queries_csv_files:\n",
    "        if not query_file.exists():\n",
    "            print(f\"ERR: {query_file} does not exist.\")\n",
    "            all_ok = False\n",
    "if (not sources_csv_files or len(sources_csv_files) < 1) and (\n",
    "    not queries_csv_files or len(queries_csv_files) < 1\n",
    "):\n",
    "    print(\"ERR: Nothing imported!\")\n",
    "if all_ok:\n",
    "    print(f\"All files found. Good to go!\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## IMPORT SOURCES (Optional)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for csv_filename in sources_csv_files:\n",
    "\n",
    "    print(f'Importing sources from \"{csv_filename}\".')\n",
    "\n",
    "    count = 0\n",
    "\n",
    "    with open(csv_filename, newline=\"\", encoding=\"utf-8\") as csvfile:\n",
    "        for source in csv.DictReader(csvfile):\n",
    "\n",
    "            parsed_source = {\n",
    "                \"name\": source[\"name\"],\n",
    "                \"title\": source[\"title\"],\n",
    "                \"webpage\": source[\"url\"],\n",
    "                \"contentType\": source.get(\"contentType\", \"website\"),\n",
    "                \"country\": source.get(\"country\", \"\"),\n",
    "                \"language\": source.get(\"language\", \"\"),\n",
    "                \"copyright\": source.get(\"copyright\", \"\")\n",
    "            }\n",
    "\n",
    "            filename = source_dir / f'{source[\"name\"]}.json'\n",
    "            with open(filename, \"w\", encoding=\"utf-8\") as jsonfile:\n",
    "                count += 1\n",
    "                json.dump(parsed_source, jsonfile, indent=4, ensure_ascii=False)\n",
    "                print(f\"- {filename}\")\n",
    "\n",
    "    print(f'Done! Found {count} sources in \"{csv_filename}\".\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## IMPORT QUERIES (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from we1s_chomp import db\n",
    "\n",
    "for csv_filename in queries_csv_files:\n",
    "\n",
    "    print(f'Importing queries from \"{csv_filename}\"...')\n",
    "\n",
    "    count = 0\n",
    "\n",
    "    with open(csv_filename, newline=\"\", encoding=\"utf-8\") as csvfile:\n",
    "        for query in csv.DictReader(csvfile):\n",
    "\n",
    "            # Create query name: source_term_startDate_endDate(.json)\n",
    "            name = \"_\".join(\n",
    "                [\n",
    "                    query[\"source\"],\n",
    "                    query[\"term\"],\n",
    "                    query[\"startDate\"].replace(\"/\", \"-\"),\n",
    "                    query[\"endDate\"].replace(\"/\", \"-\"),\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            parsed_query = {\n",
    "                \"name\": name,\n",
    "                \"title\": name,\n",
    "                \"source\": query[\"source\"],\n",
    "                \"query_str\": query[\"term\"],\n",
    "                \"start_date\": query[\"startDate\"].replace(\"/\", \"-\"),\n",
    "                \"end_date\": query[\"endDate\"].replace(\"/\", \"-\")\n",
    "            }\n",
    "\n",
    "            # Update source with query name.\n",
    "            source = db.load_source(parsed_query[\"source\"], source_dir)\n",
    "            source.queries.add(parsed_query[\"name\"])\n",
    "            db.save_source(source, source_dir)\n",
    "\n",
    "            filename = query_dir / f\"{name}.json\"\n",
    "            with open(filename, \"w\", encoding=\"utf-8\") as jsonfile:\n",
    "                count += 1\n",
    "                json.dump(parsed_query, jsonfile, indent=4, ensure_ascii=False)\n",
    "                print(f\"- {filename}\")\n",
    "\n",
    "    print(f'Done! Found {count} queries in \"{csv_filename}\".\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## NEXT NOTEBOOK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: next notebook code\n",
    "# Go to 01_responses.ipynb"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
